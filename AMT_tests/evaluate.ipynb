{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "np.random.seed(400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load test data from files\n",
    "def load_test_data(test_folder):\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    test_files = [f for f in os.listdir(test_folder) if f.endswith('_X.npy')]\n",
    "\n",
    "    for test_file in test_files:\n",
    "        base_name = test_file.replace('_X.npy', '')\n",
    "        X = np.load(os.path.join(test_folder, base_name + '_X.npy'))\n",
    "        y = np.load(os.path.join(test_folder, base_name + '_y.npy'))\n",
    "\n",
    "        X_test.append(X)\n",
    "        y_test.append(y)\n",
    "\n",
    "    return np.concatenate(X_test), np.concatenate(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply post-processing to the predictions\n",
    "def post_process_predictions(predictions):\n",
    "    for a in range(predictions.shape[1]):\n",
    "        for j in range(2, predictions.shape[0] - 3):\n",
    "            if predictions[j-1, a] == 1 and predictions[j, a] == 0 and predictions[j+1, a] == 0 and predictions[j+2, a] == 1:\n",
    "                predictions[j, a] = 1\n",
    "                predictions[j+1, a] = 1\n",
    "            if predictions[j-2, a] == 0 and predictions[j-1, a] == 0 and predictions[j, a] == 1 and predictions[j+1, a] == 1 and predictions[j+2, a] == 0 and predictions[j+3, a] == 0:\n",
    "                predictions[j, a] = 0\n",
    "                predictions[j+1, a] = 0\n",
    "            if predictions[j-1, a] == 0 and predictions[j, a] == 1 and predictions[j+1, a] == 0 and predictions[j+2, a] == 0:\n",
    "                predictions[j, a] = 0\n",
    "            if predictions[j-1, a] == 1 and predictions[j, a] == 0 and predictions[j+1, a] == 1 and predictions[j+2, a] == 1:\n",
    "                predictions[j, a] = 1\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to evaluate the model on the test data\n",
    "def evaluate_model(model, X_test, y_test, apply_post_processing=True):\n",
    "    print(f'Input shape: {X_test.shape()}')\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = np.round(predictions)  # Round to binary 0 or 1\n",
    "    # Apply post-processing if needed\n",
    "    if apply_post_processing:\n",
    "        predictions = post_process_predictions(predictions)\n",
    "\n",
    "    # Flatten the predictions and ground truth for metric calculation\n",
    "    predictions_flat = predictions.flatten()\n",
    "    y_test_flat = y_test.flatten()\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test_flat, predictions_flat)\n",
    "    precision = precision_score(y_test_flat, predictions_flat, average='macro')\n",
    "    recall = recall_score(y_test_flat, predictions_flat, average='macro')\n",
    "    f1 = f1_score(y_test_flat, predictions_flat, average='macro')\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Return the predictions and metrics\n",
    "    return predictions, {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot predictions vs ground truth\n",
    "def plot_results(predictions, y_test):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title('Predictions (Post-Processed)')\n",
    "    plt.imshow(predictions.T, cmap='Greys', aspect='auto')\n",
    "\n",
    "    # Plot ground truth\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title('Ground Truth')\n",
    "    plt.imshow(y_test.T, cmap='Greys', aspect='auto')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(model_path)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 3. Evaluate model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m predictions, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_post_processing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 4. Plot results\u001b[39;00m\n\u001b[1;32m     13\u001b[0m plot_results(predictions, y_test)\n",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_test, y_test, apply_post_processing)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, X_test, y_test, apply_post_processing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "test_folder = '/root/dev/data/paper_split/test_set1'\n",
    "model_path = '/root/dev/Deep-Neural-Networks-for-Piano-Music-Transcription/Saved_weights/DNN_3L/weights.hdf5'\n",
    "# 1. Load test data\n",
    "X_test, y_test = load_test_data(test_folder)\n",
    "\n",
    "# 2. Load model\n",
    "model = load_model(model_path)\n",
    "\n",
    "# 3. Evaluate model\n",
    "predictions, metrics = evaluate_model(model, X_test, y_test, apply_post_processing=True)\n",
    "\n",
    "# 4. Plot results\n",
    "plot_results(predictions, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
