{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import librosa\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "\n",
    "def get_file_list(audio_folder, text_folder):\n",
    "    \"\"\"\n",
    "    This function gets the base file names that are present in both the audio and text folders.\n",
    "\n",
    "    Args:\n",
    "    - audio_folder: Path to the folder containing the audio files.\n",
    "    - text_folder: Path to the folder containing the text files.\n",
    "\n",
    "    Returns:\n",
    "    - A list of base file names (without extensions) that are present in both folders.\n",
    "    \"\"\"\n",
    "    # Get the set of audio files (without extensions)\n",
    "    audio_files = set(os.path.splitext(f)[0] for f in os.listdir(audio_folder) if f.endswith('.wav'))\n",
    "\n",
    "    # Get the set of text files (without extensions)\n",
    "    text_files = set(os.path.splitext(f)[0] for f in os.listdir(text_folder) if f.endswith('.txt'))\n",
    "\n",
    "    # Find the intersection of both sets (i.e., files present in both folders)\n",
    "    common_files = list(audio_files.intersection(text_files))\n",
    "\n",
    "    return sorted(common_files)\n",
    "\n",
    "\n",
    "def split_dataset(files):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train_tr, train_va, test_set1, and test_set2.\n",
    "\n",
    "    Args:\n",
    "    - files: List of file names (without extensions).\n",
    "\n",
    "    Returns:\n",
    "    - train_tr_files, train_va_files, test_set1_files, test_set2_files\n",
    "    \"\"\"\n",
    "    virtual_instruments = {'AkPnCGdD', 'AkPnStgb', 'AkPnBcht', 'AkPnBsdf', 'SptkBGCl', 'SptkBGAm', 'StbgTGd2'}\n",
    "    real_instruments = {'ENSTDkAm', 'ENSTDkCl'}\n",
    "\n",
    "    pattern = re.compile(r'MAPS_MUS-(.*)_(\\w+)')\n",
    "    pieces_dict = defaultdict(set)\n",
    "\n",
    "    for file_name in files:\n",
    "        match = pattern.search(file_name)\n",
    "        if match:\n",
    "            piece_name = match.group(1)  # First capturing group for the piece name\n",
    "            instrument_name = match.group(2)  # Second capturing group for the instrument name\n",
    "            pieces_dict[piece_name].add(instrument_name)\n",
    "\n",
    "    # Identify seen/unseen pieces\n",
    "    unseen_pieces = {piece for piece, instruments in pieces_dict.items() if len(instruments) == 1}\n",
    "    seen_pieces = {piece for piece in pieces_dict if piece not in unseen_pieces}\n",
    "\n",
    "    # Initialize file lists for each set\n",
    "    train_tr_files, train_va_files, test_set1_files, test_set2_files = [], [], [], []\n",
    "\n",
    "    # Step 1: Split files\n",
    "    for file_name in files:\n",
    "        match = pattern.search(file_name)\n",
    "        if match:\n",
    "            piece_name = match.group(1)\n",
    "            instrument_name = match.group(2)\n",
    "\n",
    "            if instrument_name in virtual_instruments:\n",
    "                if piece_name in seen_pieces:\n",
    "                    train_tr_files.append(file_name)\n",
    "                elif piece_name in unseen_pieces:\n",
    "                    train_va_files.append(file_name)\n",
    "            elif instrument_name in real_instruments:\n",
    "                if piece_name in unseen_pieces:\n",
    "                    test_set1_files.append(file_name)\n",
    "                test_set2_files.append(file_name)\n",
    "\n",
    "    return train_tr_files, train_va_files, test_set1_files, test_set2_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_files(audio_folder, text_folder, split_file_list, output_folder, sr=16000, hop_length=512, bins_per_octave=36, n_bins=252, frames_per_file=40000, batch_size=5000, normalization=None):\n",
    "    \"\"\"\n",
    "    Preprocess audio files by applying CQT, aligning labels, concatenating frames of the same song,\n",
    "    splitting into chunks of frames (default 40,000) and saving as sequentially numbered files.\n",
    "    Mini-batches of 5000 frames are used for training, with silence-only chunks removed.\n",
    "\n",
    "    Args:\n",
    "    - audio_folder: Path to the folder containing the original audio files.\n",
    "    - text_folder: Path to the folder containing the original text files.\n",
    "    - split_file_list: List of files to process for a specific data split (train, val, test).\n",
    "    - output_folder: Folder where the processed files should be saved.\n",
    "    - sr: Target sample rate for downsampling.\n",
    "    - hop_length: Hop length for CQT.\n",
    "    - bins_per_octave: Number of bins per octave for CQT.\n",
    "    - n_bins: Total number of bins for CQT (default 252 for 7 octaves).\n",
    "    - frames_per_file: Number of frames per output file (adjusted for sample rate if needed).\n",
    "    - batch_size: Number of frames per mini-batch (default 5000).\n",
    "    - normalization: Tuple of (mean, min, max) for normalization, or None for training set.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    mean_X, min_X, max_X = [], [], []\n",
    "\n",
    "    # Initialize concatenated lists\n",
    "    concatenated_cqt = []\n",
    "    concatenated_labels = []\n",
    "\n",
    "    # File counter for naming the output files sequentially\n",
    "    file_counter = 0\n",
    "\n",
    "    for file_base in split_file_list:\n",
    "        audio_file = os.path.join(audio_folder, f\"{file_base}.wav\")\n",
    "        text_file = os.path.join(text_folder, f\"{file_base}.txt\")\n",
    "\n",
    "        if not os.path.exists(audio_file) or not os.path.exists(text_file):\n",
    "            print(f\"Missing files for {file_base}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load audio and convert to mono\n",
    "        y, sr_orig = librosa.load(audio_file, sr=sr, mono=True)\n",
    "\n",
    "        # Apply CQT\n",
    "        cqt_features = np.abs(librosa.cqt(y, sr=sr, hop_length=hop_length, n_bins=n_bins, bins_per_octave=bins_per_octave)).T\n",
    "        num_frames = cqt_features.shape[0]\n",
    "\n",
    "        # Vector of time stamps\n",
    "        win_len = hop_length / float(sr)\n",
    "        vector_aux = np.arange(1, num_frames + 1) * win_len\n",
    "\n",
    "        # Align labels with CQT frames\n",
    "        labels = np.zeros((num_frames, 88))  # 88 piano keys (from MIDI 21 to 108)\n",
    "        with open(text_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:  # Skip empty lines\n",
    "                    continue\n",
    "                parts = line.strip().split()\n",
    "                if \"OnsetTime\" not in line and len(parts) == 3:\n",
    "                    init_range, fin_range, pitch = float(parts[0]), float(parts[1]), int(parts[2])\n",
    "                    pitch = int(pitch) - 21  # MIDI note to index (MIDI 21-108)\n",
    "\n",
    "                    index_min = np.where(vector_aux >= init_range)[0]\n",
    "                    index_max = np.where(vector_aux - 0.01 > int((fin_range) * 100) / float(100))[0]\n",
    "\n",
    "                    if len(index_min) == 0 or len(index_max) == 0:\n",
    "                        continue\n",
    "                    labels[index_min[0]:index_max[0], pitch] = 1\n",
    "\n",
    "        # Normalize features if applicable\n",
    "        if normalization:\n",
    "            min_train, max_train, _ = normalization  # Do not subtract mean if you want strict [0, 1] normalization\n",
    "            cqt_features = (cqt_features - min_train) / (max_train - min_train)  # Min-max normalization to [0, 1]\n",
    "        else:\n",
    "            min_X.append(cqt_features.min())\n",
    "            max_X.append(cqt_features.max())\n",
    "            mean_X.append(cqt_features.sum(axis=0))\n",
    "\n",
    "        # Concatenate frames and labels for this song\n",
    "        concatenated_cqt.append(cqt_features)\n",
    "        concatenated_labels.append(labels)\n",
    "\n",
    "    # Once all songs are processed, concatenate into a single large array\n",
    "    concatenated_cqt = np.concatenate(concatenated_cqt, axis=0)\n",
    "    concatenated_labels = np.concatenate(concatenated_labels, axis=0)\n",
    "\n",
    "    # Handle chunking and mini-batch splitting\n",
    "    num_chunks = len(concatenated_cqt) // frames_per_file\n",
    "    total_frames = len(concatenated_cqt)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_cqt = concatenated_cqt[i * frames_per_file: (i + 1) * frames_per_file]\n",
    "        chunk_labels = concatenated_labels[i * frames_per_file: (i + 1) * frames_per_file]\n",
    "\n",
    "        num_mini_batches = frames_per_file // batch_size\n",
    "\n",
    "        for j in range(num_mini_batches):\n",
    "            mini_cqt = chunk_cqt[j * batch_size: (j + 1) * batch_size]\n",
    "            mini_labels = chunk_labels[j * batch_size: (j + 1) * batch_size]\n",
    "\n",
    "            # Skip silent mini-batches (all-zero frames)\n",
    "            if np.all(mini_cqt == 0):\n",
    "                print(f\"Skipping silent mini-batch at chunk {i}, mini-batch {j}.\")\n",
    "                continue\n",
    "\n",
    "            # Save the mini-batch\n",
    "            np.save(os.path.join(output_folder, f\"{file_counter}_X.npy\"), mini_cqt)\n",
    "            np.save(os.path.join(output_folder, f\"{file_counter}_y.npy\"), mini_labels)\n",
    "            file_counter += 1\n",
    "\n",
    "    # Handle the remainder of the data\n",
    "    remainder_cqt = concatenated_cqt[num_chunks * frames_per_file:]\n",
    "    remainder_labels = concatenated_labels[num_chunks * frames_per_file:]\n",
    "\n",
    "    if len(remainder_cqt) > 0:\n",
    "        # Pad the remainder to the nearest multiple of 5000\n",
    "        num_remainder_frames = len(remainder_cqt)\n",
    "        pad_size = batch_size * ((num_remainder_frames + batch_size - 1) // batch_size) - num_remainder_frames\n",
    "\n",
    "        padded_cqt = np.pad(remainder_cqt, ((0, pad_size), (0, 0)), mode='constant')\n",
    "        padded_labels = np.pad(remainder_labels, ((0, pad_size), (0, 0)), mode='constant')\n",
    "\n",
    "        # Split the padded remainder into mini-batches\n",
    "        num_mini_batches = len(padded_cqt) // batch_size\n",
    "\n",
    "        for j in range(num_mini_batches):\n",
    "            mini_cqt = padded_cqt[j * batch_size: (j + 1) * batch_size]\n",
    "            mini_labels = padded_labels[j * batch_size: (j + 1) * batch_size]\n",
    "\n",
    "            if np.all(mini_cqt == 0):\n",
    "                print(f\"Skipping silent mini-batch in remainder, mini-batch {j}.\")\n",
    "                continue\n",
    "\n",
    "            # Save the remainder mini-batches\n",
    "            np.save(os.path.join(output_folder, f\"{file_counter}_X.npy\"), mini_cqt)\n",
    "            np.save(os.path.join(output_folder, f\"{file_counter}_y.npy\"), mini_labels)\n",
    "            file_counter += 1\n",
    "\n",
    "    # Return normalization values for training\n",
    "    if not normalization:\n",
    "        total_frames = sum([x.shape[0] for x in mean_X])\n",
    "        train_mean = np.sum(mean_X, axis=0) / total_frames\n",
    "        min_train = min(min_X)\n",
    "        max_train = max(max_X)\n",
    "        print(f'Train mean: {train_mean}, Min_train: {min_train}, Max_train: {max_train}')\n",
    "        return train_mean, min_train, max_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(audio_folder, text_folder, train_tr_files, train_va_files, test_set1_files, test_set2_files, output_folder):\n",
    "    \"\"\"\n",
    "    Handles preprocessing and normalizing the dataset.\n",
    "    \"\"\"\n",
    "    # Process train_tr first to get normalization parameters\n",
    "    print(\"Processing train_tr set...\")\n",
    "    train_mean, min_train, max_train = preprocess_files(audio_folder, text_folder, train_tr_files, os.path.join(output_folder, 'train_tr'))\n",
    "\n",
    "    normalization = (min_train, max_train, train_mean)\n",
    "\n",
    "    # Normalize the training set after computing normalization parameters\n",
    "    print(\"Normalizing train_tr set...\")\n",
    "    preprocess_files(audio_folder, text_folder, train_tr_files, os.path.join(output_folder, 'train_tr_normalized'), normalization=normalization)\n",
    "\n",
    "    # Process validation and test sets with normalization\n",
    "    print(\"Processing train_va set...\")\n",
    "    preprocess_files(audio_folder, text_folder, train_va_files, os.path.join(output_folder, 'train_va'), normalization=normalization)\n",
    "\n",
    "    print(\"Processing test_set1...\")\n",
    "    preprocess_files(audio_folder, text_folder, test_set1_files, os.path.join(output_folder, 'test_set1'), normalization=normalization)\n",
    "\n",
    "    print(\"Processing test_set2...\")\n",
    "    preprocess_files(audio_folder, text_folder, test_set2_files, os.path.join(output_folder, 'test_set2'), normalization=normalization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train_tr set...\n",
      "Train mean: [0.02457724 0.02529809 0.02786504 0.03002799 0.03079498 0.03267209\n",
      " 0.03368827 0.03351117 0.03332503 0.03426971 0.03673549 0.04234582\n",
      " 0.04762595 0.0425057  0.04863415 0.05898338 0.04873133 0.03821773\n",
      " 0.03713151 0.03762642 0.03512997 0.03420176 0.03457216 0.03939056\n",
      " 0.04131493 0.03959154 0.04318852 0.04799475 0.04913368 0.07063571\n",
      " 0.08748765 0.07562285 0.08489244 0.10204061 0.10850244 0.15621868\n",
      " 0.19656268 0.18472055 0.20605214 0.23337217 0.23000203 0.26452413\n",
      " 0.29238722 0.29059452 0.41652662 0.55452853 0.44770378 0.3807099\n",
      " 0.42793623 0.36022124 0.39972794 0.48097602 0.3743621  0.3845272\n",
      " 0.46538952 0.4226783  0.59597176 0.7441719  0.60925376 0.6367992\n",
      " 0.7473693  0.64782584 0.66562074 0.7969145  0.6706506  0.7637556\n",
      " 0.99923366 0.7478186  0.6015459  0.7401919  0.60178983 0.6925185\n",
      " 0.9598573  0.7034014  0.60420597 0.71232414 0.5562162  0.59739554\n",
      " 0.7230227  0.5760288  0.59792984 0.74375755 0.5571973  0.5982544\n",
      " 0.8099549  0.6414364  0.7892257  1.0421563  0.7107574  0.6668746\n",
      " 0.8453739  0.6442105  1.0038158  1.4337873  0.92993444 0.9865371\n",
      " 1.3103423  0.83206743 0.75329345 1.0770519  0.79464376 0.7371828\n",
      " 1.0540297  0.7374439  0.6395204  0.8885556  0.6248278  0.75098205\n",
      " 1.1809225  0.8045978  0.6773864  0.9574223  0.7052997  0.9219145\n",
      " 1.4548506  0.946077   0.9373974  1.4527569  0.9399646  0.9690338\n",
      " 1.5004885  0.96958184 0.96314895 1.4905871  0.9563296  0.79082155\n",
      " 1.1596638  0.77483803 0.9035854  1.489467   0.9380723  0.69136834\n",
      " 0.9806262  0.6298389  0.86658096 1.4046609  0.8668652  0.9261692\n",
      " 1.5158265  0.9249706  0.7409077  1.2309579  0.8288569  0.8315289\n",
      " 1.3328538  0.83238786 0.7315787  1.0321968  0.63964236 0.8234761\n",
      " 1.359946   0.8305698  0.7248542  1.1021178  0.6327882  0.49877477\n",
      " 0.8288451  0.5370148  0.49739516 0.83555347 0.5365003  0.34033477\n",
      " 0.49085155 0.34390852 0.42547384 0.7106376  0.4595573  0.3764265\n",
      " 0.5306428  0.32650003 0.33066097 0.52330333 0.33439645 0.30442736\n",
      " 0.494587   0.3191291  0.27519313 0.4333289  0.28347296 0.2609128\n",
      " 0.41184562 0.26846918 0.22348695 0.30707714 0.19588631 0.21157758\n",
      " 0.36219877 0.24921189 0.18507738 0.2761562  0.18580887 0.16395636\n",
      " 0.25650316 0.17660478 0.16591989 0.2334246  0.1438485  0.12968738\n",
      " 0.19551648 0.13442306 0.12840734 0.22213782 0.15927705 0.125816\n",
      " 0.18199125 0.11860803 0.10157869 0.17211576 0.12782808 0.1011713\n",
      " 0.15701024 0.10986218 0.07540282 0.12219885 0.09543052 0.06790959\n",
      " 0.11438973 0.09132083 0.06082506 0.0916727  0.06756755 0.04695445\n",
      " 0.07823245 0.06614164 0.04858904 0.07185002 0.05481644 0.03835952\n",
      " 0.05805032 0.04997892 0.03300317 0.0453473  0.04047809 0.0274133\n",
      " 0.03771132 0.03442012 0.02334984 0.03361865 0.03433383 0.02159681\n",
      " 0.02378431 0.02109232 0.01278702 0.01746203 0.01875822 0.01194865\n",
      " 0.01339268 0.01351542 0.00842684 0.00840244 0.0100346  0.00742729], Min_train: 0.0, Max_train: 4.938210487365723\n",
      "Normalizing train_tr set...\n",
      "Processing train_va set...\n",
      "Processing test_set1...\n",
      "Processing test_set2...\n"
     ]
    }
   ],
   "source": [
    "audio_folder = '/root/dev/data/audio'\n",
    "text_folder = '/root/dev/data/text'\n",
    "output_folder = '/root/dev/data/paper_split'\n",
    "\n",
    "file_list = get_file_list(audio_folder, text_folder)\n",
    "\n",
    "\n",
    "# Split dataset\n",
    "train_tr_files, train_va_files, test_set1_files, test_set2_files = split_dataset(file_list)\n",
    "\n",
    "# Preprocess and normalize dataset\n",
    "normalize_dataset(audio_folder, text_folder, train_tr_files, train_va_files, test_set1_files, test_set2_files, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
